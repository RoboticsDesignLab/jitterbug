%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

% Key links for ICRA2020
% CFP: https://www.icra2020.org/call-for-papers
% PaperPlaza: https://ras.papercept.net/conferences/scripts/start.pl
% Author Guidelines: http://ras.papercept.net/conferences/support/tex.php

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
%\documentclass[a4paper, 10pt, conference]{ieeeconf}

% This command is only needed if you want to use the \thanks command
\IEEEoverridecommandlockouts

\overrideIEEEmargins

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{booktabs}

% Some small icons we use in the text
\newcommand{\com}{\,\includegraphics[width=9pt]{ico-com}\,}
\newcommand{\hinge}{\,\includegraphics[width=9pt]{ico-hinge}\,}
\newcommand{\motor}{\,\includegraphics[width=9pt]{ico-motor}\,}

\usepackage{lipsum}

\title{
    \LARGE \bf%
    A Reduced-Order Approach to Assist with Reinforcement Learning for Underactuated Robotics
}

\author{
    J\'er\'emy Augot$^{1,2}$, Aaron J. Snoswell$^{2}$ and Surya P. N. Singh$^{2}$
    % <-this % stops a space
    \thanks{
        $^{1}$ Ecole CentraleSup\'elec, Paris, France
    }%
    \thanks{
        $^{2}$The Robotics Design Lab at The University of Queensland, Brisbane, Australia
    }%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

\lipsum[1]

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Foo bar \cite{Haarnoja2018SAC}

\lipsum[1-2]

\begin{figure}[ht]
    
    \centering
    
    \begin{minipage}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{katita}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{jitterbug}
    \end{minipage}
    
    \caption{
        The wind-up children's toy `Katita' (left) was the inspiration for our underactuated `Jitterbug' continuous control task (right).
        In the simulated robot the wind-up spring is replaced with a controlled single degree-of-freedom motor.
        %The simulated robot retains the (non functional) wind-up crank to more closely mimic the mass distribution of the real toy.
        For scale, the blue checks on the floor are 1cm in size.
    }
    \label{fig:leader}
    
\end{figure}

\lipsum[1-4]

\section{RELATED WORK}

\subsection{Reinforcement Learning}

\subsection{Deep Deterministic Policy Gradients (DDPG)}

\subsection{Advantage Actor-Critic (A2C)}

\subsection{Proximal Policy Optimization (PPO)}

\subsection{DeepMind Control Suite}

\subsection{Under-actuated Control}

\lipsum[1-2]

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig-system-arch}
    \caption{
        The architecture of our system.
    }
    \label{fig:system-arch}
\end{figure}

\section{METHOD}

\lipsum[1-9]

\section{A NOVEL, UNDER-ACTUATED CONTROL BENCHMARK}

We implemented our Jitterbug benchmark using the DeepMind Control Suite (DMC) framework [REF].
DMC is a framework and set of benchmark tasks for continuous control published by Google DeepMind in 2018.
DMC benchmarks consist of a \emph{domain} defining a robotic and environment model and \emph{tasks} which are instances of that domain with specific MDP structure

DMC uses the robust Multi-Joint dynamics with Contact (MuJoCo) robotics physics engine for simulation [REF].
To aid comparison across tasks, DMC imposes constraints on rewards ($R \in [0, 1]$) and episode length ($H = 1000$).
As such, for any DMC task, cumulative episode return $\approx 1000$ indicates success.

DMC tasks are compatible subsets of the popular OpenAI Gym framework [REF], meaning many popular RL algorithm frameworks can be used with these benchmarks.

\subsection{The Jitterbug Domain}

Our Jitterbug model was inspired by the children's toy Katita (Figure~\ref{fig:leader}).
We aimed to reproduce the physical dynamics of this toy while enabling control by replacing the wind-up spring with a single actuator of equivalent torque.
Our Jitterbug model conforms to the dimensions and mass of the Katita, however we replace the wind-up spring with a controlled single degree-of-freedom motor.
We retain the (non-functional) wind up crank to more closely model the mass distribution of the physical Katita.

We used high-speed recording and visual tachometry to measure the Katita motor speed and leg vibration modes.
By reverse-engineering the Katita gearbox we estimated the torque output of the drive spring and configured the MuJoCo actuator appropriately.
We modelled the legs as rigid bodies with shoulder and elbow hinge joints.
The hinge stiffness was manually tuned to reproduce the dominant leg vibration mode observed in our high-speed footage.
The Jitterbug model density was set using standard values for stainless steel ($7700$ kg/m$^3$) for the body and tough plastic ($1100$ kg/m$^3$) for the feet.
Figure~\ref{fig:parts} shows the physical composition of our simulated Jitterbug model.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig-jitterbug-parts}
    \caption[
        Schematic representation of the Jitterbug model.
        Individual rigid bodies are in different colours and we highlight the position of the center of mass, hinge joints and the single motor.
    ]{
        Schematic representation of the Jitterbug model.
        Individual rigid bodies are in different colours and we highlight the position of the center of mass (\protect\com), hinge joints  (\protect\hinge) and the single motor (\protect\motor).
    }
    \label{fig:parts}
\end{figure}

Due to the importance of contact and stiff dynamics in the Jitterbug's locomotion, we found it necessary to adjust MuJoCo's default settings, selecting an integration timestep of $0.0002$ and semi-implicit Euler integration.
With these settings we qualitatively observed a close correspondence between the Katita and the simulated dynamics under constant motor actuation on the Jitterbug.

DMC supports the definition of physically-based camera models for to enable learning from raw pixels if desired.
We defined several cameras for the Jitterbug domain including an overhead, tracking and ego-centric view.

\subsection{The Jitterbug Task Suite}

The Jitterbug dynamics naturally induce very high variance motion under a range of motor velocities.
We defined a collection of five tasks of increasing difficulty based on the Jitterbug domain.
The tasks were designed with increasingly sparse reward signals to increase the difficulty.

For all tasks we choose $\gamma = 0.99$ and consider a task solved when cumulative episode reward is $\gtrapprox 900$.
In all tasks the Jitterbug is reset to a random pose near the origin at the start of an episode.
All tasks have a single continuous action controlling the motor $\mathcal{A} = [-1, 1]$ (larger/smaller values are clipped) and continuous state and observation spaces.

For each task, we report ($\text{dim}(\mathcal{S}), \text{dim}(\mathcal{O})$) and a brief description of the reward structure.
Tasks are reported here ordered easiest to hardest.

\begin{enumerate}
    
    \item \emph{Move From Origin} (16, 15): The Jitterbug must move away from the origin in any direction.
    N.b. a sufficiently fast constant motor velocity is sufficient to solve this task.
    
    \item \emph{Face In Direction} (17, 16): The Jitterbug must rotate to face in a randomly selected yaw direction.
    
    \item \emph{Move In Direction} (20, 19): The Jitterbug is rewarded for velocity in a randomly selected direction in the X, Y plane.
    
    \item \emph{Move To Position} (19, 18): The Jitterbug must move to a randomly selected position in the X, Y plane.
    
    \item \emph{Move To Pose} (20, 19): The Jitterbug must move to a randomly selected position in the X, Y plane and rotate to face in a randomly selected yaw direction.
    N.b. Due to the multiplication of position and yaw reward components, this task has a \emph{very} sparse reward signal!
    
\end{enumerate}

In addition, for all tasks the Jitterbug must remain upright to achieve reward.
Falling does not terminate the episode early as the leg dynamics are sufficiently springy that bouncing into the upright pose again can allow recovery from this condition (albeit at the loss of some reward).
Indeed - we observed some learned strategies that appeared to utilize this mode of locomotion!

\section{EXPERIMENTS}

\subsection{Characterising The Jitterbug Tasks}

To verify feasibility, we hand-crafted heuristic policies that can solve each task.
To characterise the difficulty of the Jitterbug task suite, we performed preliminary hyper-parameter tuning to select reasonable settings and trained several RL algorithms on the tasks.

Figure~\ref{fig:rl-perf} reports training curves for example on- and off-policy algorithms.
We contrast the performance of PPO (an on-policy method) and DDPG (an off-policy method).
We also overlay the performance of our heuristic policies for comparison.
Each figure shows the median and 10\textsuperscript{th} - 90\textsuperscript{th} percentile episode return across 10 different seeds.

Our selected hyper-parameters are reported in Table~\ref{tab:results}.
For all cases, we used fully-connected neural networks with hidden layers of size 350 and 250 with ReLU activation.
Where applicable, we use separate networks for the actor and critic (i.e. no shared weights).

We ran additional experiments using TRPO, A2C and SAC and observed similar performance to the reported results.
Training curves for these algorithms are not included here for brevity.

\begin{table*}[t!]
    
    \centering
    \caption{Algorithm Hyper-parameters}
    \label{tab:results}
    \medskip
    
    {\def\arraystretch{1.2}
        \begin{tabularx}{\textwidth}{X rr c rr c rr}
            
            \toprule
            
            & \multicolumn{2}{c}{\% Distance Match} & \phantom{a} & \multicolumn{2}{c}{Feature Vector L2} & \phantom{a} & \multicolumn{2}{c}{Log Likelihood} \\
            
            \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
            
            & {avg.} & {med.} & & {avg.} & {med.} & & {avg.} & {med.} \\
            
            \midrule
            
            MaxEnt (Ours) &
            56.74 & 57.52 &  & 5.145E3 & 1.270E3 &  & 4.360E-121 & 2.932e-121 \\
            
            MaxEnt \cite{Ziebart2008} &
            38.03 & 31.36 &  & 172.406E3 & 178.043E3 &  & $-\infty$ & $-\infty$ \\
            
            MaxEnt (Length-binned ensemble) &
            60.12 & 62.13 &  & 3.388E3 & 1.662E3 &  & 1.050e-14 & 8.733e-15 \\
            
            MaxEnt ($k$-means ensemble) &
            65.58 & 73.68 &  & 1.132E3 & 562.29 &  & 3.74E-119 & 2.8228e-119 \\[10pt]
            
            
            Shortest Path &
            52.10 & 50.54 &  & 2.679E3 & 995.400 &  & {-} & {-} \\
            
            Logistic Regression &
            52.45 & 50.18 &  & {-} & {-} &  & {-} & {-} \\
            
            \bottomrule
            
        \end{tabularx}
    }
    
\end{table*}

\subsection{Characterising Learned Policies}

To verify the learned policies were sensible (i.e. to confirm the absence of `reward hacking') we qualitatively and quantitatively investigated the exhibited behaviours.

We observed that a key difference between successful and unsuccessful trained policies seemed to be the ability to learn piecewise control functions.
For example, for all tasks but \emph{Move From Origin}, achieving high reward requires careful modulation of the reactive torque applied to the Jitterbug body by the motor counterweight.
One way to achieve this (the method we use in our heuristic policies) is by pulsing the motor in different directions.
We observed that successful policies learned to pulse the motor in short bursts in alternating directions (e.g. $\sim180\degree$ at a time, see Figure~\ref{fig:motor-hist}), whereas unsuccessful policies would often drive the motor continuously.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig-motor-hist}
    \caption{
        Characterising successful policy behaviours for the \emph{Move In Direction} task.
        We plot the distribution of motor velocities across many episodes for a successful policy (green) and unsuccessful policy (orange).
        The successful policy learns to pulse the motor in alternating directions (the same strategy used by our heuristic policy).
        In contrast, the unsuccessful policy gets stuck in a local minima where the motor is continuously driven in one direction.
    }
    \label{fig:motor-hist}
\end{figure}

\subsection{Reduced-Order Training}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig-autoencoder}
    \caption{
        WE used a De-Noising AutoEncoder as a means to learn a reduced-order state representation.
    }
    \label{fig:autoencoder}
\end{figure}

\lipsum[1-9]

\begin{figure*}[p]
    
    \centering
    \includegraphics[width=\textwidth]{fig-rl-perf}
    
    \captionsetup{margin=10pt}
    \caption{
        Comparison of RL algorithms over environment training steps.
        We show median (solid line) and the first and third quartiles (shaded area) across 10 random seeds in each figure.
        All plots are filtered with a $10e^3$ step moving average filter.
    }
    
    \label{fig:rl-perf}
\end{figure*}

\section{DISCUSSION}

\lipsum[1]

\section{CONCLUSION}

\lipsum[1]

%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

\lipsum[1]

\section*{ACKNOWLEDGMENT}

\lipsum[1]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{ieeetr}
\bibliography{root}

\end{document}
