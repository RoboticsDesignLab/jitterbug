% Encoding: UTF-8


@article{Haarnoja2018SAC,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}


@article{DDPG,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}


@inproceedings{todorov2008general,
  title={General duality between optimal control and estimation},
  author={Todorov, Emanuel},
  booktitle={IEEE Conference on Decision and Control},
  pages={4286--4292},
  year={2008},
  organization={IEEE}
}


@inproceedings{bhatnagar2009convergent,
  title={Convergent temporal-difference learning with arbitrary smooth function approximation},
  author={Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S and Maei, Hamid R and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1204--1212},
  year={2009}
}


@article{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and van Hoof, Herke and Meger, David},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}


@article{gu2016q,
  title={Q-prop: Sample-efficient policy gradient with an off-policy critic},
  author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.02247},
  year={2016}
}


@inproceedings{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@inproceedings{gupta2017cooperative,
  title={Cooperative multi-agent control using deep reinforcement learning},
  author={Gupta, Jayesh K and Egorov, Maxim and Kochenderfer, Mykel},
  booktitle={International Conference on Autonomous Agents and Multiagent Systems},
  pages={66--83},
  year={2017},
  organization={Springer}
}


@article{arulkumaran2017deep,
  title={Deep reinforcement learning: A brief survey},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={26--38},
  year={2017},
  publisher={IEEE}
}


@article{kimura2018daqn,
  title={DAQN: Deep auto-encoder and Q-network},
  author={Kimura, Daiki},
  journal={arXiv preprint arXiv:1806.00630},
  year={2018}
}


@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={784--800},
  year={2018}
}


@inproceedings{wang2019haq,
  title={HAQ: Hardware-Aware Automated Quantization with Mixed Precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8612--8620},
  year={2019}
}

@inproceedings{nagabandi2018learning,
  title={Learning Image-Conditioned Dynamics Models for Control of Underactuated Legged Millirobots},
  author={Nagabandi, Anusha and Yang, Guangzhao and Asmar, Thomas and Pandya, Ravi and Kahn, Gregory and Levine, Sergey and Fearing, Ronald S},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={4606--4613},
  year={2018},
  organization={IEEE}
}


@article{clavera2018learning,
  title={Learning to adapt: Meta-learning for model-based control},
  author={Clavera, Ignasi and Nagabandi, Anusha and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1803.11347},
  volume={3},
  year={2018}
}


@inproceedings{morton2018deep,
  title={Deep dynamical modeling and control of unsteady fluid flows},
  author={Morton, Jeremy and Jameson, Antony and Kochenderfer, Mykel J and Witherden, Freddie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9258--9268},
  year={2018}
}


@article{lutter2019deep,
  title={Deep Lagrangian Networks for end-to-end learning of energy-based control for under-actuated systems},
  author={Lutter, Michael and Listmann, Kim and Peters, Jan},
  journal={arXiv preprint arXiv:1907.04489},
  year={2019}
}


@article{Hongwei.USV,
author = {Hongwei Xu and Ning Wang and Hong Zhao and Zhongjiu Zheng},
title = {Deep reinforcement learning-based path planning of underactuated surface vessels},
journal = {Cyber-Physical Systems},
volume = {5},
number = {1},
pages = {1-17},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/23335777.2018.1540018},

URL = { 
        https://doi.org/10.1080/23335777.2018.1540018
    
},
eprint = { 
        https://doi.org/10.1080/23335777.2018.1540018
    
}
,
    abstract = {This paper is dedicated to the issues of path planning for the underactuated surface vessel (USV) in unknown environments with obstacles. Aiming at the usability problem caused by the complicated control law of the traditional method, a deep deterministic policy gradient (DDPG)-based path planning algorithm is proposed with the powerful actor-critic architecture. The main contributions of this paper are as follows: (1) The DDPG-based path planning method is the decision control system of USV whose output is continuous. (2) A complete reward system is specially designed for target approaching, speed control and attitude correction. (3) Since the reward system is independent, it is highly customisable and can be changed according to the task and the control model. Through the simulation, the results show that the perfect path can be automatically generated under unknown environmental disturbance, thus providing the proposed DDPG-based path planning scheme effectiveness and applied meaning. }
}


]@inproceedings{zhang2018deep,
  title={Deep imitation learning for complex manipulation tasks from virtual reality teleoperation},
  author={Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}


@inproceedings{fan2018surreal,
  title={Surreal: Open-source reinforcement learning framework and robot manipulation benchmark},
  author={Fan, Linxi and Zhu, Yuke and Zhu, Jiren and Liu, Zihua and Zeng, Orien and Gupta, Anchit and Creus-Costa, Joan and Savarese, Silvio and Fei-Fei, Li},
  booktitle={Conference on Robot Learning},
  pages={767--782},
  year={2018}
}


@article{lynch2019learning,
  title={Learning Latent Plans from Play},
  author={Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre},
  journal={arXiv preprint arXiv:1903.01973},
  year={2019}
}


@InProceedings{Wang_ae_vision_feature_fusion,
author="Wang, Haotian
and Yang, Wenjing
and Huang, Wanrong
and Lin, Zhipeng
and Tang, Yuhua",
editor="Cheng, Long
and Leung, Andrew Chi Sing
and Ozawa, Seiichi",
title="Multi-feature Fusion for Deep Reinforcement Learning: Sequential Control of Mobile Robots",
booktitle="Neural Information Processing",
year="2018",
publisher="Springer International Publishing",
pages="303--315",
abstract="Compared with traditional motion planners, deep reinforcement learning has been applied more and more widely to achieving sequential behaviours control of mobile robots in indoor environment. However, the state of robot in deep reinforcement learning is commonly obtained through single sensor, which lacks accuracy and stability. In this paper, we propose a novel approach called multi-feature fusion framework. The multi-feature fusion framework utilizes multiple sensors to gather different scene images around the robot. Once environment information is gathered, a well-trained autoencoder achieves the fusion and extraction of multiple visual features. With more accurate and stable states extracted from the autoencoder, we train the mobile robot to patrol and navigate in 3D simulation environment with an asynchronous deep reinforcement learning algorithm. Extensive simulation experiments demonstrate that the proposed multi-feature fusion framework improves not only the convergence rate of training phase but also the testing performance of the mobile robot.",
isbn="978-3-030-04239-4"
}


@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}


@article{recht2019tour,
  title={A tour of reinforcement learning: The view from continuous control},
  author={Recht, Benjamin},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  volume={2},
  pages={253--279},
  year={2019},
  publisher={Annual Reviews}
}


@inproceedings{james2019sim,
  title={Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks},
  author={James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={12627--12637},
  year={2019}
}

@Article{Islam2017,
  author  = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
  title   = {Reproducibility of benchmarked deep reinforcement learning tasks for continuous control},
  journal = {arXiv preprint arXiv:1708.04133},
  year    = {2017},
}

@InProceedings{Mnih2016,
  author    = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  title     = {Asynchronous methods for deep reinforcement learning},
  booktitle = {International conference on machine learning},
  year      = {2016},
  pages     = {1928--1937},
}

@Comment{jabref-meta: databaseType:bibtex;}
